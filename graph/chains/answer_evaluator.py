from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from langchain_core.runnables import RunnableSequence

llm = ChatOpenAI(model="gpt-5-mini", temperature=0.0)

class EvaluatedAnswer(BaseModel):
    confidence_score: float = Field(..., description="A score between 0 and 1 indicating the confidence level of the generated answer")
    feedback: str = Field(..., description="Constructive feedback to improve the quality of the generated answer")

structured_eval = llm.with_structured_output(EvaluatedAnswer)

system = """
    You are a highly professional grader who evaluates the quality of the answer generated by an assistant in response to a user's question.
    Your task is to assess the quality of the generated answer based on the following criteria:
    1. Relevance: How well does the answer address the user's question?
    2. Accuracy: Is the information provided in the answer correct and supported by the provided documents?
    3. Completeness: Does the answer cover all aspects of the user's question?
    4. Clarity: Is the answer clearly and coherently presented?
    Based on these criteria, assign a confidence score between 0 and 1 to the generated answer, where 0 indicates a poor quality answer and 1 indicates an excellent quality answer.
    Also provide a constructive feedback that highlights the strengths and weaknesses of the generated answer and offers specific suggestions for improvement.
"""

eval_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "User Question: {question} \n\n Generated Answer: {generation} \n\n Web Search Results: {documents}")
    ]
)

eval_chain: RunnableSequence = eval_prompt | structured_eval